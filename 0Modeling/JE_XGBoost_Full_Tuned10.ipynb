{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from collections import Counter\n",
    "from scipy.special import expit, logit\n",
    "\n",
    "# pip install xgboost\n",
    "# pip install graphviz\n",
    "# conda install python-graphviz\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# below was for the CI\n",
    "# from functools import partial\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.inspection import partial_dependence\n",
    "from sklearn.inspection import plot_partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Type Model] You have chosen Full_Cov\n",
      "[Checking Calibration] Matched Coordinators\n",
      "[Sampling Grid] Training Grids: 166 ----- Validating Grids: 30 ----- Testing Grids: 30\n",
      "===== Sampling Model =====\n",
      "[Saving] Save Grid Index\n",
      "[Saving] Done Saving Grid Index\n",
      "[Preprocessing] Total Training containing NA: 0 / 488914 ----- 0.0%\n",
      "[Preprocessing] Total Validating containing NA: 0 / 89152 ----- 0.0%\n",
      "[Preprocessing] Total Testing containing NA: 0 / 123240 ----- 0.0%\n",
      "[Preprocessing] Total EndemicDF containing NA: 0 / 896827 ----- 0.0%\n"
     ]
    }
   ],
   "source": [
    "# ==================== Define functions ====================\n",
    "\n",
    "def Extract_Features(PD, Type = 'Full_Cov'): # Functions that only extract interesting features, remove unrelated columns in the pandas dataframe\n",
    "    # Input PD is the entire pandas dataframe read from csv (result from Rds files of R)\n",
    "    # Type is an approach that you want to run\n",
    "    #   - Full_Cov: Keep all features --> remove x-y coordinates and FOI column\n",
    "    #   - Only_Bio: Only keep Bioclimatic Features --> remove x-y coordinates, FOI column, and other not-bioclimatic columns\n",
    "    # Output is an pandas dataframe containing only interesting features columns\n",
    "    # Note: You need to check the names of columns that you want to remove\n",
    "    \n",
    "    if (Type == 'Full_Cov'):\n",
    "        # Full Covariates --> Using all covariates --> remove x-y coordinates and FOI values columns (keep all features)\n",
    "        if ('FOI' in PD.columns): # This check is for Endemic dataframe since it does not have FOI column    \n",
    "            Interest_Features = PD.drop(['x', 'y', 'FOI'], axis = 1)\n",
    "        else:\n",
    "            Interest_Features = PD.drop(['x', 'y'], axis = 1)\n",
    "    else:\n",
    "        # Only Bio Covariates --> Only using bioclimatic --> remove x-y coordinates, FOI columns, and other columns that are not bioclimatic\n",
    "        if ('FOI' in PD.columns): \n",
    "            Interest_Features = PD.drop(['x', 'y', 'DG_000_014bt_dens', 'Elv', 'Pigs', 'Pop_Count_WP_SEDAC_2015', 'Rice', 'UR', 'VD', 'FOI'], axis = 1)\n",
    "        else:\n",
    "            Interest_Features = PD.drop(['x', 'y', 'DG_000_014bt_dens', 'Elv', 'Pigs', 'Pop_Count_WP_SEDAC_2015', 'Rice', 'UR', 'VD'], axis = 1)\n",
    "    return Interest_Features\n",
    "\n",
    "np.random.seed(5) # set seed to regenerate the same random result\n",
    "\n",
    "\n",
    "# ==================== Set up parameters and Read data ====================\n",
    "\n",
    "Typemodel = 'Full_Cov' # if Full Cov model --> More detail in above Extract_Features functions\n",
    "#Typemodel = 'Only_Bio' # if only bio model \n",
    "\n",
    "Train_Portion = 0.7 # Portion of Train - Validate - Test\n",
    "Validate_Portion = (1 - Train_Portion) / 2\n",
    "\n",
    "resolution_grid = 400\n",
    "Name_Grid_File = 'Grid_' + str(resolution_grid) + '_' + str(resolution_grid) + '.csv'\n",
    "\n",
    "print('[Type Model] You have chosen ' + Typemodel)\n",
    "\n",
    "CurDir = os.getcwd() + '/'\n",
    "Data_All = CurDir + 'Generate/Python_CSV/EM_Imputed_Features_Study.csv' # 'Directory/to/your/EM_Imputed_Features_Study.csv'\n",
    "Grid = CurDir + 'Generate/Grids_CSV/' + Name_Grid_File # 'Directory/to/your/Grid/' + Name_Grid_File\n",
    "Data_EndemicDF = CurDir + 'Generate/Python_CSV/Imputed_Features_Endemic.csv' # 'Directory/to/your/Imputed_Features_Endemic.csv'\n",
    "\n",
    "# Directory to the folder that you want this script exports files to (remember to have slash '/' at the end)\n",
    "# Default is to create a subfolder named Python_Export and save result to that subfolder   \n",
    "Savepath = CurDir + 'Generate/Python_Export/' #@@\n",
    "if not os.path.exists(Savepath):\n",
    "    os.makedirs(Savepath)\n",
    "\n",
    "# Read csv and store in dataframe in pandas\n",
    "AllData = pd.read_csv(Data_All)\n",
    "Grid = pd.read_csv(Grid)  \n",
    "EndemicDF = pd.read_csv(Data_EndemicDF)\n",
    "EndemicDF = EndemicDF.drop(['FOI'], axis = 1) # remove FOI column (if it has)\n",
    "\n",
    "# Remove Pop_2015 density (if wanted, since already have Pop_Count people) --> Check feature name again\n",
    "# AllData = AllData.drop(['Pop_2015'], axis = 1)\n",
    "# EndemicDF = EndemicDF.drop(['Pop_2015'], axis = 1)\n",
    "\n",
    "# Check if matching coordinator\n",
    "if (len(AllData.iloc[:, :2].merge(Grid.iloc[:, :2])) == len(AllData.iloc[:, :2])):\n",
    "    print('[Checking Calibration] Matched Coordinators')\n",
    "    # Recreate grid to match with AllData in case of nrow of 2 dataframe is different\n",
    "    t = pd.merge(AllData.iloc[:, : 2].reset_index(), Grid.iloc[:, : 2].reset_index(), on=['x','y'], suffixes=['_1','_2'])\n",
    "    Grid = Grid.iloc[t['index_2'], :]\n",
    "    del t\n",
    "else:\n",
    "    sys.exit('[Stop Program] Grid and Data File do not match coordinators --> Check again')\n",
    "\n",
    "\n",
    "# ==================== Sampling Grids to define which Grids will be for Train-Validate-Test ====================\n",
    "\n",
    "# Count freq of pix in each grid\n",
    "Grid_column = Grid.iloc[:, 2]\n",
    "Grid_column = np.array(Grid_column)\n",
    "d = Counter(Grid_column)\n",
    "grid_freq = np.array(list(d.values())) # number of pix in each grid_numb (belowed)\n",
    "grid_numb = np.array(list(d.keys()))\n",
    "del d\n",
    "\n",
    "# ----- Preprocessing for Sampling train and validate -----\n",
    "idx_grid_numb_less = np.where(grid_freq < 100)[0] # find idx of grid containing less than 100 pix --> these grids will be automaticly in training set\n",
    "idx_grid_numb_high = np.where(grid_freq >= 100)[0] # find idx of grid containing more than 100 pix --> these grids will be randomly chosen for training\n",
    "\n",
    "grid_numb_train_1 = grid_numb[idx_grid_numb_less]\n",
    "grid_numb_sample = grid_numb[idx_grid_numb_high]\n",
    "\n",
    "ngrid_train_2 = round(len(grid_numb_sample)*0.7) # 70% train --- 30% test (validate)\n",
    "ngrid_validate = round(len(grid_numb_sample)*0.15)\n",
    "ngrid_test = len(grid_numb_sample) - ngrid_train_2 - ngrid_validate\n",
    "ngrid_train = len(grid_numb_train_1) + ngrid_train_2\n",
    "\n",
    "print('[Sampling Grid] Training Grids: ' + str(ngrid_train) + ' ----- Validating Grids: ' + str(ngrid_validate) + ' ----- Testing Grids: ' + str(ngrid_test))\n",
    "\n",
    "print('===== Sampling Model =====')    \n",
    "grid_numb_sample_shuffle = np.copy(grid_numb_sample)\n",
    "np.random.shuffle(grid_numb_sample_shuffle)\n",
    "grid_numb_train_2 = grid_numb_sample_shuffle[:ngrid_train_2]\n",
    "grid_numb_validate = grid_numb_sample_shuffle[ngrid_train_2:(ngrid_train_2 + ngrid_validate)]\n",
    "grid_numb_test = grid_numb_sample_shuffle[(ngrid_train_2 + ngrid_validate):]\n",
    "grid_numb_train = np.append(grid_numb_train_1, grid_numb_train_2)\n",
    "del grid_numb_sample_shuffle, grid_numb_train_2\n",
    "\n",
    "# ----- Take index of pixel for each sub-dataset\n",
    "idx_train = np.where(np.in1d(Grid_column, grid_numb_train))[0]\n",
    "idx_validate = np.where(np.in1d(Grid_column, grid_numb_validate))[0]\n",
    "idx_test = np.where(np.in1d(Grid_column, grid_numb_test))[0]\n",
    "\n",
    "Type = np.zeros(AllData.shape[0])\n",
    "Type[idx_train] = 1 # index 1 for train\n",
    "Type[idx_validate] = 2 # index 2 for validate\n",
    "Type[idx_test] = 3 # index 3 for validate\n",
    "\n",
    "# Saving Sampling Grid index having 3 columns: x-y coordinates, and Grid index\n",
    "Coor_Grid_Index = AllData.iloc[:, 0:2]\n",
    "Coor_Grid_Index = Coor_Grid_Index.assign(Type = pd.Series(Type).values)\n",
    "filename_grid = 'Grid_Index_' + str(resolution_grid) + '.csv'\n",
    "print('[Saving] Save Grid Index')\n",
    "Coor_Grid_Index.to_csv(Savepath + filename_grid, sep='\\t', encoding='utf-8')\n",
    "print('[Saving] Done Saving Grid Index')\n",
    "\n",
    "\n",
    "# ==================== Create Train-Validate-Test dataset ====================\n",
    "\n",
    "# ===== Prepare Train =====\n",
    "Train = AllData.iloc[idx_train, :]\n",
    "row_na = Train.isnull().any(1) # check whether a row contains NA or not\n",
    "#idx_row_na = row_na.nonzero()[0] # find index of row containing NA\n",
    "#SJ: AttributeError: 'Series' object has no attribute 'nonzero'. This method is deprecated. See below\n",
    "idx_row_na = row_na.to_numpy().nonzero()[0] # find index of row containing NA\n",
    "\n",
    "print('[Preprocessing] Total Training containing NA: ' + str(len(idx_row_na)) + ' / ' + str(len(Train)) + ' ----- ' + \n",
    "      str(round(len(idx_row_na) / len(Train) * 100, 2)) + '%')\n",
    "Train_Non_NA = Train.drop(Train.index[idx_row_na]) # remove row containing NA\n",
    "\n",
    "# ----- Extract Features for model -----\n",
    "X_train = Extract_Features(Train_Non_NA, Typemodel)\n",
    "Y_train = Train_Non_NA.FOI\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "# ===== Prepare Validate =====\n",
    "Validate = AllData.iloc[idx_validate, :]\n",
    "row_na = Validate.isnull().any(1) # check whether a row contains NA or not\n",
    "#idx_row_na = row_na.nonzero()[0] # find index of row containing NA\n",
    "#SJ: AttributeError: 'Series' object has no attribute 'nonzero'. This method is deprecated. See below\n",
    "idx_row_na = row_na.to_numpy().nonzero()[0] # find index of row containing NA\n",
    "\n",
    "print('[Preprocessing] Total Validating containing NA: ' + str(len(idx_row_na)) + ' / ' + str(len(Validate)) + ' ----- ' + \n",
    "      str(round(len(idx_row_na) / len(Validate) * 100, 2)) + '%')\n",
    "Validate_Non_NA = Validate.drop(Validate.index[idx_row_na]) # remove row containing NA\n",
    "\n",
    "# ----- Extract Features for model -----\n",
    "X_validate = Extract_Features(Validate_Non_NA, Typemodel)\n",
    "Y_validate = Validate_Non_NA.FOI\n",
    "Y_validate = np.array(Y_validate)\n",
    "\n",
    "# ===== Prepare Test =====\n",
    "Test = AllData.iloc[idx_test, :]\n",
    "row_na = Test.isnull().any(1) # check whether a row contains NA or not\n",
    "#idx_row_na = row_na.nonzero()[0] # find index of row containing NA\n",
    "#SJ: AttributeError: 'Series' object has no attribute 'nonzero'. This method is deprecated. See below\n",
    "idx_row_na = row_na.to_numpy().nonzero()[0] # find index of row containing NA\n",
    "\n",
    "print('[Preprocessing] Total Testing containing NA: ' + str(len(idx_row_na)) + ' / ' + str(len(Test)) + ' ----- ' + \n",
    "      str(round(len(idx_row_na) / len(Test) * 100, 2)) + '%')\n",
    "Test_Non_NA = Validate.drop(Test.index[idx_row_na]) # remove row containing NA\n",
    "\n",
    "# ----- Extract Features for model -----\n",
    "X_test = Extract_Features(Test_Non_NA, Typemodel)\n",
    "Y_test = Test_Non_NA.FOI\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# ===== Prepare EndemicDF =====\n",
    "row_na = EndemicDF.isnull().any(1) # check whether a row contains NA or not\n",
    "#idx_row_na = row_na.nonzero()[0] # find index of row containing NA\n",
    "#SJ: AttributeError: 'Series' object has no attribute 'nonzero'. This method is deprecated. See below\n",
    "idx_row_na = row_na.to_numpy().nonzero()[0] # find index of row containing NA\n",
    "\n",
    "print('[Preprocessing] Total EndemicDF containing NA: ' + str(len(idx_row_na)) + ' / ' + str(len(EndemicDF)) + ' ----- ' + \n",
    "      str(round(len(idx_row_na) / len(EndemicDF) * 100, 2)) + '%')\n",
    "EndemicDF_Non_NA = EndemicDF.drop(EndemicDF.index[idx_row_na]) # remove row containing NA\n",
    "\n",
    "# ----- Extract Features for model -----\n",
    "X_endemic = Extract_Features(EndemicDF_Non_NA, Typemodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============Encode the categorical variables==============\n",
    "le = LabelEncoder()\n",
    "le.fit(pd.concat([X_train,X_validate, X_test]).UR)\n",
    "X_train['UR'] = le.transform(X_train.UR.values)\n",
    "X_validate['UR'] = le.transform(X_validate.UR.values)\n",
    "X_test['UR'] = le.transform(X_test.UR.values)\n",
    "\n",
    "m_name = 'XGB'\n",
    "print('===== Training XGBoost =====')\n",
    "\n",
    "# ==============Hyperparameter Tuning with XGBoost\n",
    "\n",
    "# Random Search\n",
    "\n",
    "# xgb_param_grid = {\n",
    "#     'n_estimators': range(100,1001,100),\n",
    "#     'max_depth': range(2,11,2),\n",
    "#     'eta': [0.0001, 0.0005, 0.001, 0.005, 0.01,0.1,0.2,0.3]\n",
    "# }\n",
    "# # Instantiate the xg_regessor: xgb\n",
    "# xgb = XGBRegressor()\n",
    "# # Perform random search: grid_mse\n",
    "# randomized_mse = RandomizedSearchCV(param_distributions=xgb_param_grid, \n",
    "#                                     estimator=xgb, \n",
    "#                                     scoring=\"neg_mean_squared_error\",  \n",
    "#                                     cv=4, #kfolds\n",
    "#                                     verbose=1,\n",
    "#                                     n_jobs = -1)\n",
    "# # Fit randomized_mse to the data\n",
    "# randomized_mse.fit(X_train, Y_train) \n",
    "# # Print the best parameters and lowest RMSE\n",
    "# # print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "\n",
    "# bestp = randomized_mse.best_params_\n",
    "# print(\"Best parameters found: \", bestp)\n",
    "\n",
    "# Result of the previous run on 100% of data\n",
    "bestp = {'n_estimators': 1000,\n",
    "        'max_depth': 8,\n",
    "        'eta': 0.005}\n",
    "\n",
    "# save best parameters to csv\n",
    "bestp_pd = pd.DataFrame(list(bestp.items()), columns = ['Parameters','Value'])\n",
    "filename_bestp = 'Bestp_' + m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "bestp_pd.to_csv(Savepath + filename_bestp, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# ============== Refit XGBoost with the optimal params\n",
    "\n",
    "xg_reg = XGBRegressor(objective='reg:linear', \n",
    "                          n_estimators = bestp['n_estimators'],\n",
    "                          eta = bestp['eta'],\n",
    "                          seed=123)\n",
    "\n",
    "# Fit the xg_regessor to the training set\n",
    "eval_set = [(X_train, Y_train),(X_validate, Y_validate)]\n",
    "start_time = time.time()\n",
    "xg_reg.fit(X_train, Y_train, eval_metric = 'rmse', eval_set = eval_set, verbose = False)\n",
    "end_time = time.time()\n",
    "print('[Training] Finish training')\n",
    "training_time = round(end_time - start_time, 5) # seconds\n",
    "print('Training Time: ' + str(training_time) + ' seconds')\n",
    "\n",
    "yhat = xg_reg.predict(X_test)\n",
    "\n",
    "# ============== Save the model\n",
    "# filename_model = Savepath + 'ModelXGB_' + '.model'\n",
    "# print('[Saving] Save training model')\n",
    "# pickle.dump(xg_reg, open(filename_model, 'wb'))\n",
    "\n",
    "# You can load the model by the following way\n",
    "\n",
    "# filename_model = Savepath + 'ModelXGB_' + '.model'\n",
    "# xg_reg = pickle.load(open(filename_model, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Export csv of TestDF (Coor and result)\n",
    "print('[Saving] Save predicted test FOI with coords')\n",
    "Coor = Test_Non_NA.iloc[:, 0: 2]\n",
    "Coor = Coor.assign(Actual = pd.Series(Y_test).values,\n",
    "                   Predict = pd.Series(yhat).values)\n",
    "filename_test = 'Test_FOI_'+ m_name +'_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "Coor.to_csv(Savepath + filename_test, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# plot\n",
    "dat = np.concatenate((Y_test.reshape(-1,1), \n",
    "                      yhat.reshape(-1,1)),axis = 1)\n",
    "dat = pd.DataFrame(dat, columns = ['Actual','Predict'])\n",
    "# sort the dataframe for plotting\n",
    "dat2 = dat.sort_values('Actual')\n",
    "\n",
    "# take 1% and replot the above CI\n",
    "dat2 = dat2.sample(frac = 0.01, replace = False, random_state = 123)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "plt.scatter(dat2['Actual'], dat2['Predict'],label = 'Predicted test FOI')\n",
    "plt.plot([0.05,0.45],[0.05,0.45], 'k--')\n",
    "# plt.errorbar(dat2['Actual'], dat2['Predict'], np.array(dat2['Lower'], dat2['Upper']), fmt='o',alpha = 0.5)\n",
    "\n",
    "plt.xlabel('Actual FOI')\n",
    "plt.ylabel('Predicted FOI')\n",
    "plt.ylim(0,0.5)\n",
    "plot_name = m_name\n",
    "\n",
    "plt.title(plot_name)\n",
    "plt.legend()\n",
    "plt.rc('font',size = 15)\n",
    "plt.rc('axes',labelsize = 15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save thinned plot \n",
    "print('[Saving] Save thinned test/predicted FOI plot')\n",
    "filename_ci_plot = 'FOI_'+ m_name +'_' + Typemodel + '_' + str(resolution_grid) + '.png'\n",
    "plt.savefig(Savepath + filename_ci_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training XGBoost =====\n",
      "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-afe9cb5dca78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                     n_jobs = -1)\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Fit randomized_mse to the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mrandomized_mse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m# Print the best parameters and lowest RMSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# print(\"Best parameters found: \", randomized_mse.best_params_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[1;32m   1620\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m             random_state=self.random_state))\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ============== 1. MSE\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_reg.predict(X_test)\n",
    "mse_train = mean_squared_error(Y_train, xg_reg.predict(X_train))\n",
    "mse_test = mean_squared_error(Y_test, preds) #@@\n",
    "print(\"train MSE: %f\" % (mse_train))\n",
    "print(\"test MSE: %f\" % (mse_test))\n",
    "\n",
    "# Export csv of MSE\n",
    "print('[Saving] Save MSE evaluation')        \n",
    "Result_pd = pd.DataFrame(data = {'mse_train':[mse_train], 'mse_test':[mse_test], 'time_train':[training_time]})   \n",
    "filename_mse = 'MSE_' + m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "Result_pd.to_csv(Savepath + filename_mse, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# after training, call .evals_result()\n",
    "results = xg_reg.evals_result()\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('XGBoost')\n",
    "plt.plot(x_axis, [m**2 for m in results['validation_0']['rmse']], 'b-',\n",
    "         label='Training Set MSE')\n",
    "plt.plot(x_axis, [m**2 for m in results['validation_1']['rmse']], 'r-',\n",
    "         label='Validate Set MSE')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting iterations')\n",
    "plt.ylabel('MSE')\n",
    "plt.rc('font',size = 15)\n",
    "plt.rc('axes',labelsize = 15)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save MSE plot \n",
    "print('[Saving] Save MSE plot')\n",
    "filename_mse_plot = 'MSE_' + m_name + '_'+ Typemodel + '_' + str(resolution_grid) + '.png'\n",
    "plt.savefig(Savepath + filename_mse_plot)\n",
    "\n",
    "# ----- Export csv of TestDF (Coor and result)\n",
    "print('[Saving] Save predicted test FOI with coords')\n",
    "Coor = Test_Non_NA.iloc[:, 0: 2]\n",
    "Coor = Coor.assign(Predict = pd.Series(yhat).values)\n",
    "filename_test = 'Test_FOI_'+ m_name +'_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "Coor.to_csv(Savepath + filename_test, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============== 1. get feature importances\n",
    "# it is the same API interface like for ‘scikit-learn’ models\n",
    "feature_importance = xg_reg.feature_importances_\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "fig = plt.figure(figsize=(6, 10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, np.array(X_test.columns)[sorted_idx])\n",
    "plt.title('XGBoost')\n",
    "plt.xlabel('Feature Importance Scores')\n",
    "plt.rc('font',size = 15)\n",
    "plt.rc('axes',labelsize = 15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save feature importance and plot \n",
    "print('[Saving] Save variable importance ranking plot')\n",
    "filename_varimp_plot = 'Varimp' + m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.png'\n",
    "plt.savefig(Savepath + filename_varimp_plot)\n",
    "\n",
    "print('[Saving] Save variable importance ranking')\n",
    "data = {'Name': X_endemic.columns, 'Importance': feature_importance}\n",
    "importance_df = pd.DataFrame(data)\n",
    "# importance std not saved here\n",
    "importance_df = importance_df.sort_values(by = 'Importance', ascending = False)\n",
    "filename_varimp = 'Varimp' + m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "importance_df.to_csv(Savepath + filename_varimp, sep='\\t', encoding='utf-8')\n",
    "\n",
    "# list of top 10 features \n",
    "\n",
    "top10 = np.flip(np.array(X_train.columns)[sorted_idx])[:10]\n",
    "print('Top 10 features are: ', top10)\n",
    "\n",
    "# Export top10\n",
    "print('[Saving] Save top10 features')        \n",
    "Result_pd = pd.DataFrame(data = {'top10':top10})      \n",
    "filename_mse = 'top10_' + m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "Result_pd.to_csv(Savepath + filename_mse, sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "# ============== 2. get dependence plots\n",
    "# Partial dependence plots\n",
    "_, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.set_title( 'XGBoost')\n",
    "display2 = plot_partial_dependence(xg_reg,X_test, top10[:3],ax = ax)\n",
    "display2.axes_[0][0].set_ylabel('Predicted Test FOI')\n",
    "\n",
    "# Save partial dependence plots\n",
    "print('[Saving] Save partial dependence plots')\n",
    "filename_pd_plot = 'PDP' + m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.png'\n",
    "ax.figure.savefig(Savepath + filename_pd_plot)\n",
    "\n",
    "# ICE plots\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "display3 = plot_partial_dependence(xg_reg,X_test, top10[:3],ax = ax, kind = 'both', subsample = 100)\n",
    "ax.set_title( 'XGBoost')\n",
    "display3.axes_[0][0].set_ylabel('Predicted Test FOI')\n",
    "\n",
    "# Save ICE plots \n",
    "print('[Saving] Save individual conditional expectation plots')\n",
    "filename_ice_plot = 'ICE_'+ m_name + '_' + Typemodel + '_' + str(resolution_grid) + '.png'\n",
    "ax.figure.savefig(Savepath + filename_ice_plot)\n",
    "\n",
    "# ============== Prediction without interval\n",
    "\n",
    "preds_endem = xg_reg.predict(X_endemic)\n",
    "\n",
    "# ----- Export csv of EndemicDF (Coor and result)\n",
    "print('[Saving] Save predicted FOI with coords')\n",
    "Coor = EndemicDF_Non_NA.iloc[:, 0: 2]\n",
    "Coor = Coor.assign(Predict = pd.Series(preds_endem).values)\n",
    "filename_endemic = 'Endemic_FOI_XGB_' + Typemodel + '_' + str(resolution_grid) + '.csv'\n",
    "Coor.to_csv(Savepath + filename_endemic, sep='\\t', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
